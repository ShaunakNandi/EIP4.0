# Assignment 3 - training the CIFAR10 dataset

  1. under 100K params
  2. beat base validation accuracy: 84%
  3. No Conv2D (implement spatially separable convolutions)
  
Please click [here](https://colab.research.google.com/drive/1L3Za-g1VnY2ejCDMfiHiN68U5cvQFBaq) if the colab page does not render
  
# Model Definition

>> Layer(type)                 Output Shape              Param #
=================================================================
input_mod2 (InputLayer)      (None, None, None, 3)     0         
_________________________________________________________________
conv_0 (SeparableConv2D)     (None, None, None, 16)    75        
_________________________________________________________________
act0 (Activation)            (None, None, None, 16)    0         
_________________________________________________________________
drop0 (Dropout)              (None, None, None, 16)    0         
_________________________________________________________________
bn0 (BatchNormalization)     (None, None, None, 16)    64        
_________________________________________________________________
conv_1 (SeparableConv2D)     (None, None, None, 32)    656       
_________________________________________________________________
act1 (Activation)            (None, None, None, 32)    0         
_________________________________________________________________
drop1 (Dropout)              (None, None, None, 32)    0         
_________________________________________________________________
bn1 (BatchNormalization)     (None, None, None, 32)    128       
_________________________________________________________________
conv_2 (SeparableConv2D)     (None, None, None, 32)    1312      
_________________________________________________________________
act2 (Activation)            (None, None, None, 32)    0         
_________________________________________________________________
drop2 (Dropout)              (None, None, None, 32)    0         
_________________________________________________________________
bn2 (BatchNormalization)     (None, None, None, 32)    128       
_________________________________________________________________
conv_3 (SeparableConv2D)     (None, None, None, 64)    2336      
_________________________________________________________________
act3 (Activation)            (None, None, None, 64)    0         
_________________________________________________________________
drop3 (Dropout)              (None, None, None, 64)    0         
_________________________________________________________________
bn3 (BatchNormalization)     (None, None, None, 64)    256       
_________________________________________________________________
conv_5 (SeparableConv2D)     (None, None, None, 128)   8768      
_________________________________________________________________
act5 (Activation)            (None, None, None, 128)   0         
_________________________________________________________________
drop5 (Dropout)              (None, None, None, 128)   0         
_________________________________________________________________
bn5 (BatchNormalization)     (None, None, None, 128)   512       
_________________________________________________________________
trans1 (Conv2D)              (None, None, None, 16)    2048      
_________________________________________________________________
MP1 (MaxPooling2D)           (None, None, None, 16)    0         
_________________________________________________________________
conv_6 (SeparableConv2D)     (None, None, None, 16)    400       
_________________________________________________________________
act6 (Activation)            (None, None, None, 16)    0         
_________________________________________________________________
drop6 (Dropout)              (None, None, None, 16)    0         
_________________________________________________________________
bn6 (BatchNormalization)     (None, None, None, 16)    64        
_________________________________________________________________
conv_7 (SeparableConv2D)     (None, None, None, 32)    656       
_________________________________________________________________
act7 (Activation)            (None, None, None, 32)    0         
_________________________________________________________________
drop7 (Dropout)              (None, None, None, 32)    0         
_________________________________________________________________
bn7 (BatchNormalization)     (None, None, None, 32)    128       
_________________________________________________________________
conv_8 (SeparableConv2D)     (None, None, None, 32)    1312      
_________________________________________________________________
act8 (Activation)            (None, None, None, 32)    0         
_________________________________________________________________
drop8 (Dropout)              (None, None, None, 32)    0         
_________________________________________________________________
bn8 (BatchNormalization)     (None, None, None, 32)    128       
_________________________________________________________________
conv_9 (SeparableConv2D)     (None, None, None, 64)    2336      
_________________________________________________________________
act9 (Activation)            (None, None, None, 64)    0         
_________________________________________________________________
drop9 (Dropout)              (None, None, None, 64)    0         
_________________________________________________________________
bn9 (BatchNormalization)     (None, None, None, 64)    256       
_________________________________________________________________
conv_10 (SeparableConv2D)    (None, None, None, 128)   8768      
_________________________________________________________________
act10 (Activation)           (None, None, None, 128)   0         
_________________________________________________________________
drop10 (Dropout)             (None, None, None, 128)   0         
_________________________________________________________________
bn10 (BatchNormalization)    (None, None, None, 128)   512       
_________________________________________________________________
trans3 (SeparableConv2D)     (None, None, None, 10)    1418      
_________________________________________________________________
output (GlobalAveragePooling (None, 10)                0         
_________________________________________________________________
softmax (Activation)         (None, 10)                0         
=================================================================
Total params: 32,261
Trainable params: 31,173
Non-trainable params: 1,088
_________________________________________________________________

# Output

  1. channel size: 256 * 32 * 32 * 64 * 128
  2. receptive field: 32 * 32

# Log 

>> Epoch 1/50
781/781 [==============================] - 67s 86ms/step - loss: 2.2718 - acc: 0.1469 - val_loss: 2.1870 - val_acc: 0.1829

Epoch 2/50
781/781 [==============================] - 62s 80ms/step - loss: 2.0832 - acc: 0.2177 - val_loss: 1.9384 - val_acc: 0.2543

Epoch 3/50
781/781 [==============================] - 62s 79ms/step - loss: 1.9198 - acc: 0.2690 - val_loss: 1.7965 - val_acc: 0.3161

Epoch 4/50
781/781 [==============================] - 62s 79ms/step - loss: 1.8367 - acc: 0.3034 - val_loss: 1.7478 - val_acc: 0.3503

Epoch 5/50
781/781 [==============================] - 62s 80ms/step - loss: 1.7727 - acc: 0.3310 - val_loss: 1.6484 - val_acc: 0.3957

Epoch 6/50
781/781 [==============================] - 62s 80ms/step - loss: 1.7160 - acc: 0.3614 - val_loss: 1.5667 - val_acc: 0.4260

Epoch 7/50
781/781 [==============================] - 62s 79ms/step - loss: 1.6725 - acc: 0.3789 - val_loss: 1.5859 - val_acc: 0.4284

Epoch 8/50
781/781 [==============================] - 62s 80ms/step - loss: 1.6300 - acc: 0.3993 - val_loss: 1.4454 - val_acc: 0.4732

Epoch 9/50
781/781 [==============================] - 62s 79ms/step - loss: 1.5889 - acc: 0.4160 - val_loss: 1.5309 - val_acc: 0.4453

Epoch 10/50
781/781 [==============================] - 62s 80ms/step - loss: 1.5527 - acc: 0.4310 - val_loss: 1.5643 - val_acc: 0.4371

Epoch 11/50
781/781 [==============================] - 62s 79ms/step - loss: 1.5227 - acc: 0.4461 - val_loss: 1.3442 - val_acc: 0.5100

Epoch 12/50
781/781 [==============================] - 62s 79ms/step - loss: 1.5013 - acc: 0.4532 - val_loss: 1.3372 - val_acc: 0.5188

Epoch 13/50
781/781 [==============================] - 62s 79ms/step - loss: 1.4767 - acc: 0.4646 - val_loss: 1.3161 - val_acc: 0.5177

Epoch 14/50
781/781 [==============================] - 62s 80ms/step - loss: 1.4552 - acc: 0.4687 - val_loss: 1.2870 - val_acc: 0.5365

Epoch 15/50
781/781 [==============================] - 62s 79ms/step - loss: 1.4341 - acc: 0.4819 - val_loss: 1.2736 - val_acc: 0.5372

Epoch 16/50
781/781 [==============================] - 62s 80ms/step - loss: 1.4134 - acc: 0.4881 - val_loss: 1.2104 - val_acc: 0.5643

Epoch 17/50
781/781 [==============================] - 62s 80ms/step - loss: 1.4018 - acc: 0.4903 - val_loss: 1.3150 - val_acc: 0.5287

Epoch 18/50
781/781 [==============================] - 62s 80ms/step - loss: 1.3859 - acc: 0.4989 - val_loss: 1.1696 - val_acc: 0.5800

Epoch 19/50
781/781 [==============================] - 62s 80ms/step - loss: 1.3715 - acc: 0.5062 - val_loss: 1.3467 - val_acc: 0.5145

Epoch 20/50
781/781 [==============================] - 62s 79ms/step - loss: 1.3587 - acc: 0.5123 - val_loss: 1.2280 - val_acc: 0.5652

Epoch 21/50
781/781 [==============================] - 62s 79ms/step - loss: 1.3477 - acc: 0.5133 - val_loss: 1.1879 - val_acc: 0.5764

Epoch 22/50
781/781 [==============================] - 62s 79ms/step - loss: 1.3323 - acc: 0.5172 - val_loss: 1.2243 - val_acc: 0.5647

Epoch 23/50
781/781 [==============================] - 62s 79ms/step - loss: 1.3269 - acc: 0.5217 - val_loss: 1.1618 - val_acc: 0.5815

Epoch 24/50
781/781 [==============================] - 62s 79ms/step - loss: 1.3102 - acc: 0.5273 - val_loss: 1.0979 - val_acc: 0.6059

Epoch 25/50
781/781 [==============================] - 62s 79ms/step - loss: 1.3030 - acc: 0.5308 - val_loss: 1.3127 - val_acc: 0.5442

Epoch 26/50
781/781 [==============================] - 62s 79ms/step - loss: 1.2928 - acc: 0.5365 - val_loss: 1.2659 - val_acc: 0.5588

Epoch 27/50
781/781 [==============================] - 62s 79ms/step - loss: 1.2838 - acc: 0.5369 - val_loss: 1.0958 - val_acc: 0.6031

Epoch 28/50
781/781 [==============================] - 62s 79ms/step - loss: 1.2787 - acc: 0.5393 - val_loss: 1.2971 - val_acc: 0.5487

Epoch 29/50
781/781 [==============================] - 62s 79ms/step - loss: 1.2756 - acc: 0.5441 - val_loss: 1.1247 - val_acc: 0.5975

Epoch 30/50
781/781 [==============================] - 62s 79ms/step - loss: 1.2593 - acc: 0.5463 - val_loss: 1.1590 - val_acc: 0.5896

Epoch 31/50
781/781 [==============================] - 62s 80ms/step - loss: 1.2532 - acc: 0.5492 - val_loss: 1.0591 - val_acc: 0.6196

Epoch 32/50
781/781 [==============================] - 62s 80ms/step - loss: 1.2443 - acc: 0.5528 - val_loss: 1.0448 - val_acc: 0.6233

Epoch 33/50
781/781 [==============================] - 62s 80ms/step - loss: 1.2524 - acc: 0.5511 - val_loss: 1.1086 - val_acc: 0.6027

Epoch 34/50
781/781 [==============================] - 62s 79ms/step - loss: 1.2345 - acc: 0.5575 - val_loss: 1.0677 - val_acc: 0.6152

Epoch 35/50
781/781 [==============================] - 62s 80ms/step - loss: 1.2276 - acc: 0.5594 - val_loss: 1.2527 - val_acc: 0.5653

Epoch 36/50
781/781 [==============================] - 63s 80ms/step - loss: 1.2187 - acc: 0.5602 - val_loss: 1.0645 - val_acc: 0.6221

Epoch 37/50
781/781 [==============================] - 63s 80ms/step - loss: 1.2238 - acc: 0.5618 - val_loss: 1.1333 - val_acc: 0.6021

Epoch 38/50
781/781 [==============================] - 63s 81ms/step - loss: 1.2097 - acc: 0.5652 - val_loss: 1.0342 - val_acc: 0.6297

Epoch 39/50
781/781 [==============================] - 63s 80ms/step - loss: 1.2096 - acc: 0.5660 - val_loss: 1.0614 - val_acc: 0.6227

Epoch 40/50
781/781 [==============================] - 63s 80ms/step - loss: 1.2042 - acc: 0.5665 - val_loss: 1.0258 - val_acc: 0.6285

Epoch 41/50
781/781 [==============================] - 63s 81ms/step - loss: 1.1918 - acc: 0.5731 - val_loss: 1.0134 - val_acc: 0.6409

Epoch 42/50
781/781 [==============================] - 63s 80ms/step - loss: 1.1950 - acc: 0.5719 - val_loss: 1.0477 - val_acc: 0.6260

Epoch 43/50
781/781 [==============================] - 63s 81ms/step - loss: 1.1854 - acc: 0.5752 - val_loss: 1.1548 - val_acc: 0.5926

Epoch 44/50
781/781 [==============================] - 63s 81ms/step - loss: 1.1806 - acc: 0.5772 - val_loss: 1.0930 - val_acc: 0.6113

Epoch 45/50
781/781 [==============================] - 63s 80ms/step - loss: 1.1736 - acc: 0.5787 - val_loss: 1.0078 - val_acc: 0.6457

Epoch 46/50
781/781 [==============================] - 63s 80ms/step - loss: 1.1724 - acc: 0.5806 - val_loss: 1.0257 - val_acc: 0.6310

Epoch 47/50
781/781 [==============================] - 62s 80ms/step - loss: 1.1690 - acc: 0.5814 - val_loss: 1.1975 - val_acc: 0.5752

Epoch 48/50
781/781 [==============================] - 62s 80ms/step - loss: 1.1638 - acc: 0.5832 - val_loss: 1.1721 - val_acc: 0.5863

Epoch 49/50
781/781 [==============================] - 62s 80ms/step - loss: 1.1637 - acc: 0.5848 - val_loss: 1.0118 - val_acc: 0.6428

Epoch 50/50
781/781 [==============================] - 62s 80ms/step - loss: 1.1542 - acc: 0.5864 - val_loss: 1.0286 - val_acc: 0.6383

